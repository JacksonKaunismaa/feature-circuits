{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/can/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from nnsight import LanguageModel\n",
    "import datasets\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from loading_utils import load_submodule_and_dictionary, DictionaryCfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize 20 datasets of the pile\n",
    "`quanta-discovery/misc/create_pile_canonical.py`\n",
    "\n",
    "## Evaluate model loss\n",
    "`quanta-discovery/scripts/evaluate_pile_losses.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize dataset with Tokens of low loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pythia-70m-deduped\"\n",
    "step = 143000\n",
    "device = \"cuda:0\"\n",
    "cache_dir = \"/home/can/feature_clustering/cache/\"\n",
    "pile_canonical = \"/home/can/data/pile_test_tokenized_200k/\"\n",
    "loss_threshold = 0.0001\n",
    "skip = 1\n",
    "num_tokens = 10000\n",
    "block_len = 250\n",
    "output_dir = \"/home/can/feature_clustering/results/\"\n",
    "filter = None\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "particular_model_cache_dir = os.path.join(cache_dir, model_name, f\"step{step}\")\n",
    "losses_cached = [f for f in os.listdir(particular_model_cache_dir) if f.endswith(\"losses.pt\")]\n",
    "max_i = max(list(range(len(losses_cached))), key=lambda i: int(losses_cached[i].split(\"_\")[0]))\n",
    "docs, tokens = int(losses_cached[max_i].split(\"_\")[0]), int(losses_cached[max_i].split(\"_\")[2])\n",
    "losses = torch.load(os.path.join(particular_model_cache_dir, f\"{docs}_docs_{tokens}_tokens_losses.pt\"))\n",
    "c = 1 / np.log(2) # for nats to bits conversion\n",
    "\n",
    "if filter:\n",
    "    criterias = torch.load(filter)\n",
    "    token_idxs = ((losses < (loss_threshold / c)) & (~criterias)).nonzero().flatten()\n",
    "else:\n",
    "    token_idxs = (losses < (loss_threshold / c)).nonzero().flatten()\n",
    "token_idxs = token_idxs[::skip]\n",
    "token_idxs = token_idxs[:num_tokens].tolist()\n",
    "assert len(token_idxs) == num_tokens, \"not enough tokens meeting loss threshold (and filter) to sample from\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1169481"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel('EleutherAI/pythia-70m-deduped', device_map='cuda:0')\n",
    "\n",
    "# Load all dictionaries\n",
    "\n",
    "dictionary_dir = \"/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped\"\n",
    "dictionary_size = 32768\n",
    "submodule, dictionary = load_submodule_and_dictionary(\n",
    "    model, \n",
    "    submod_name='model.gpt_neox.layers.5.mlp.dense_4h_to_h',\n",
    "    dict_cfg=DictionaryCfg(dictionary_dir, dictionary_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Roman Catholic Diocese of Tambacounda\\n\\nThe Roman Catholic Diocese of Tambacounda () is a diocese located in the city of Tambacounda in the Ecclesiastical province of Dakar in Senegal.\\n\\nHistory\\n August 13, 1970: Established as Apostolic Prefecture of Tambacounda from the Diocese of Kaolack and Diocese of Saint-Louis du Sénégal\\n April 17, 1989: Promoted as Diocese of Tambacounda\\n\\nSpecial churches\\n The cathedral is Cathédrale Marie Reine de l’Univers in Tambacounda, which is located in the Medina Coura neighborhood of the town.\\n\\nLeadership\\n Bishops of Tambacounda (Roman rite)\\n Bishop Jean-Noël Diouf (since 1989.04.17)\\n Prefects Apostolic of Tambacounda (Roman rite) \\n Fr. Clément Cailleau, C.S.Sp. (1970.08.13 – 1986.04.24)\\n\\nSee also\\nRoman Catholicism in Senegal\\n\\nReferences\\n\\nExternal links\\n GCatholic.org\\n Catholic Hierarchy \\n\\nCategory:Roman Catholic dioceses in Senegal\\nCategory:Tambacounda\\nCategory:Christian organizations established in 1970\\nCategory:Roman Catholic dioceses and prelatures established in the 20th century', 'meta': {'pile_set_name': 'Wikipedia (en)'}, 'input_ids': [[30574, 10503, 6129, 32143, 273, 308, 1369, 317, 517, 66, 187, 187, 510, 8370, 10503, 6129, 32143, 273, 308, 1369, 317, 517, 66, 6734, 310, 247, 1073, 32143, 4441, 275, 253, 2846, 273, 308, 1369, 317, 517, 66, 275, 253, 24253, 49431, 474, 14254, 273, 20469, 274, 275, 4673, 42552, 15, 187, 187, 12697, 187, 4223, 2145, 13, 10333, 27, 35211, 1428, 347, 38254, 3422, 5729, 42459, 273, 308, 1369, 317, 517, 66, 432, 253, 6129, 32143, 273, 15366, 311, 471, 285, 6129, 32143, 273, 11877, 14, 34830, 3443, 322, 33678, 9896, 187, 4162, 1722, 13, 11161, 27, 13798, 4225, 347, 6129, 32143, 273, 308, 1369, 317, 517, 66, 187, 187, 22166, 18777, 187, 380, 36368, 310, 8842, 860, 5267, 1079, 21088, 1720, 460, 372, 298, 457, 44920, 275, 308, 1369, 317, 517, 66, 13, 534, 310, 4441, 275, 253, 3050, 1758, 28424, 376, 9168, 273, 253, 3874, 15, 187, 187, 28749, 4249, 187, 378, 29194, 273, 308, 1369, 317, 517, 66, 313, 30574, 391, 614, 10, 187, 19520, 13089, 14, 2302, 10093, 77, 6129, 276, 71, 313, 17480, 11161, 15, 2125, 15, 1166, 10, 187, 5729, 738, 84, 38254, 3422, 273, 308, 1369, 317, 517, 66, 313, 30574, 391, 614, 10, 2490, 1879, 15, 1639, 38889, 330, 36826, 1952, 13, 330, 15, 52, 15, 6104, 15, 313, 20686, 15, 2904, 15, 1012, 1108, 12140, 15, 2125, 15, 1348, 10, 187, 187, 5035, 671, 187, 30574, 10503, 1204, 275, 4673, 42552, 187, 187, 4941, 187, 187, 7504, 4859, 187, 15679, 506, 3422, 15, 2061, 187, 10503, 39452, 12638, 2490, 187, 1413, 27, 30574, 10503, 1073, 406, 265, 265, 275, 4673, 42552, 187, 1413, 27, 53, 1369, 317, 517, 66, 187, 1413, 27, 26207, 8889, 4232, 275, 10333, 187, 1413, 27, 30574, 10503, 1073, 406, 265, 265, 285, 638, 77, 2478, 4232, 275, 253, 1384, 394, 5331]], 'split_by_token': ['Roman', ' Catholic', ' Di', 'ocese', ' of', ' T', 'amb', 'ac', 'ound', 'a', '\\n', '\\n', 'The', ' Roman', ' Catholic', ' Di', 'ocese', ' of', ' T', 'amb', 'ac', 'ound', 'a', ' ()', ' is', ' a', ' di', 'ocese', ' located', ' in', ' the', ' city', ' of', ' T', 'amb', 'ac', 'ound', 'a', ' in', ' the', ' Ec', 'clesiast', 'ical', ' province', ' of', ' Dak', 'ar', ' in', ' Sen', 'egal', '.', '\\n', '\\n', 'History', '\\n', ' August', ' 13', ',', ' 1970', ':', ' Establ', 'ished', ' as', ' Apost', 'olic', ' Pre', 'fecture', ' of', ' T', 'amb', 'ac', 'ound', 'a', ' from', ' the', ' Di', 'ocese', ' of', ' Ka', 'ol', 'ack', ' and', ' Di', 'ocese', ' of', ' Saint', '-', 'Louis', ' du', ' S', 'éné', 'gal', '\\n', ' April', ' 17', ',', ' 1989', ':', ' Prom', 'oted', ' as', ' Di', 'ocese', ' of', ' T', 'amb', 'ac', 'ound', 'a', '\\n', '\\n', 'Special', ' churches', '\\n', ' The', ' cathedral', ' is', ' Cath', 'é', 'dr', 'ale', ' Marie', ' Re', 'ine', ' de', ' l', '’', 'Univers', ' in', ' T', 'amb', 'ac', 'ound', 'a', ',', ' which', ' is', ' located', ' in', ' the', ' Med', 'ina', ' Cou', 'ra', ' neighborhood', ' of', ' the', ' town', '.', '\\n', '\\n', 'Lead', 'ership', '\\n', ' B', 'ishops', ' of', ' T', 'amb', 'ac', 'ound', 'a', ' (', 'Roman', ' r', 'ite', ')', '\\n', ' Bishop', ' Jean', '-', 'No', 'ë', 'l', ' Di', 'ou', 'f', ' (', 'since', ' 1989', '.', '04', '.', '17', ')', '\\n', ' Pre', 'fect', 's', ' Apost', 'olic', ' of', ' T', 'amb', 'ac', 'ound', 'a', ' (', 'Roman', ' r', 'ite', ')', ' \\n', ' Fr', '.', ' Cl', 'ément', ' C', 'aille', 'au', ',', ' C', '.', 'S', '.', 'Sp', '.', ' (', '1970', '.', '08', '.', '13', ' –', ' 1986', '.', '04', '.', '24', ')', '\\n', '\\n', 'See', ' also', '\\n', 'Roman', ' Catholic', 'ism', ' in', ' Sen', 'egal', '\\n', '\\n', 'References', '\\n', '\\n', 'External', ' links', '\\n', ' GC', 'ath', 'olic', '.', 'org', '\\n', ' Catholic', ' Hier', 'archy', ' \\n', '\\n', 'Category', ':', 'Roman', ' Catholic', ' di', 'oc', 'es', 'es', ' in', ' Sen', 'egal', '\\n', 'Category', ':', 'T', 'amb', 'ac', 'ound', 'a', '\\n', 'Category', ':', 'Christian', ' organizations', ' established', ' in', ' 1970', '\\n', 'Category', ':', 'Roman', ' Catholic', ' di', 'oc', 'es', 'es', ' and', ' pre', 'l', 'atures', ' established', ' in', ' the', ' 20', 'th', ' century'], 'tokens_len': 306, 'preds_len': 305}\n"
     ]
    }
   ],
   "source": [
    "# Get context for token\n",
    "dataset = datasets.load_from_disk(pile_canonical)\n",
    "\n",
    "total_tokens_in_past_docs = 0\n",
    "for doc in dataset:\n",
    "    dataset_len = doc['tokens_len']\n",
    "    token_doc_idxs = token_idxs[(tokens >= total_tokens_in_past_docs) & (tokens < total_tokens_in_past_docs + dataset_len)]\n",
    "    if token_doc_idxs.shape[0] > 0:\n",
    "        print(f\"Found {token_doc_idxs.shape[0]} tokens in doc {doc['doc_idx']}\")\n",
    "        # Do in NNsight\n",
    "        with model.invoke(doc['input_ids'].to(device)) as invoker:\n",
    "            \n",
    "        tok = doc['input_ids'].to(device)\n",
    "        logits = model(tok)\n",
    "        # Cache all feature activations in forward pass\n",
    "\n",
    "        # Cache all feature gradients in backward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results for ~10K tokens\n",
    "- feature activation\n",
    "- gradient of correct logit w.r.t feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metric as final token logit\n",
    "def metric_fn(model):\n",
    "    logits = model.embed_out.output\n",
    "    batch_size = logits.shape[0]\n",
    "    return logits[t.arange(batch_size), -1, prompt_batch_final_tok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
