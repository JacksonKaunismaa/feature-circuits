{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from dictionary_learning import AutoEncoder\n",
    "from attribution import patching_effect\n",
    "from nnsight import LanguageModel\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "model = LanguageModel(model_name, dispatch=True, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Args = namedtuple('args', ['dict_id', 'd_model', 'dict_path', 'device'])\n",
    "Ccfg = namedtuple('ccfg', ['dict_size'])\n",
    "args = Args(d_model=512, dict_id=10, device=device, dict_path=\"/share/projects/dictionary_circuits/autoencoders/pythia-70m-deduped\")\n",
    "ccfg = Ccfg(dict_size=512*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = model.gpt_neox.embed_in\n",
    "attns = [layer.attention for layer in model.gpt_neox.layers]\n",
    "mlps = [layer.mlp for layer in model.gpt_neox.layers]\n",
    "resids = [layer for layer in model.gpt_neox.layers]\n",
    "submodules = [embed] + attns + mlps + resids\n",
    "# save n_submodules to the config\n",
    "# ccfg.n_submodules = 3 * model.config.num_hidden_layers + 1\n",
    "# with open(os.path.join(ccfg.results_dir, \"config.json\"), \"w\") as f:\n",
    "#     json.dump(ccfg.__dict__, f)\n",
    "\n",
    "dictionaries = {}\n",
    "if args.dict_id == 'id':\n",
    "    from dictionary_learning.dictionary import IdentityDict\n",
    "    dictionaries[embed] = IdentityDict(args.d_model)\n",
    "    for i in range(len(model.gpt_neox.layers)):\n",
    "        dictionaries[attns[i]] = IdentityDict(args.d_model)\n",
    "        dictionaries[mlps[i]] = IdentityDict(args.d_model)\n",
    "        dictionaries[resids[i]] = IdentityDict(args.d_model)\n",
    "else:\n",
    "    for i in range(len(model.gpt_neox.layers)):\n",
    "        ae = AutoEncoder(args.d_model, ccfg.dict_size).to(args.device)\n",
    "        ae.load_state_dict(t.load(os.path.join(args.dict_path, f'embed/{args.dict_id}_{ccfg.dict_size}/ae.pt')))\n",
    "        dictionaries[embed] = ae\n",
    "\n",
    "        ae = AutoEncoder(args.d_model, ccfg.dict_size).to(args.device)\n",
    "        ae.load_state_dict(t.load(os.path.join(args.dict_path, f'attn_out_layer{i}/{args.dict_id}_{ccfg.dict_size}/ae.pt')))\n",
    "        dictionaries[attns[i]] = ae\n",
    "\n",
    "        ae = AutoEncoder(args.d_model, ccfg.dict_size).to(args.device)\n",
    "        ae.load_state_dict(t.load(os.path.join(args.dict_path, f'mlp_out_layer{i}/{args.dict_id}_{ccfg.dict_size}/ae.pt')))\n",
    "        dictionaries[mlps[i]] = ae\n",
    "\n",
    "        ae = AutoEncoder(args.d_model, ccfg.dict_size).to(args.device)\n",
    "        ae.load_state_dict(t.load(os.path.join(args.dict_path, f'resid_out_layer{i}/{args.dict_id}_{ccfg.dict_size}/ae.pt')))\n",
    "        dictionaries[resids[i]] = ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512]) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n"
     ]
    }
   ],
   "source": [
    "is_tuple = {}\n",
    "with model.trace(\"_\"):\n",
    "    for submodule in submodules:\n",
    "        is_tuple[submodule] = type(submodule.output.shape) == tuple\n",
    "        print(submodule.output.shape, 'hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contexts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcontexts\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'contexts' is not defined"
     ]
    }
   ],
   "source": [
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512]) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "torch.Size([1, 1, 512]) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n",
      "(torch.Size([1, 1, 512]), (torch.Size([1, 8, 1, 64]), torch.Size([1, 8, 1, 64]))) hi\n"
     ]
    }
   ],
   "source": [
    "contexts = \"The dog\"\n",
    "answers = \"is\"\n",
    "contexts = model.tokenizer(contexts, return_tensors=\"pt\", padding=False, truncation=True, max_length=10).to(device)\n",
    "answers = model.tokenizer(answers, return_tensors=\"pt\", padding=False, truncation=True, max_length=10).to(device)\n",
    "contexts = contexts.input_ids\n",
    "answers = answers.input_ids\n",
    "\n",
    "\n",
    "def metric_fn(model):\n",
    "        return (\n",
    "            -1 * t.gather(\n",
    "                t.nn.functional.log_softmax(model.embed_out.output[:,-1,:], dim=-1), dim=-1, index=answers.view(-1, 1)\n",
    "            ).squeeze(-1)\n",
    "        )\n",
    "\n",
    "effect_out = patching_effect(\n",
    "        clean=contexts,\n",
    "        patch=None,\n",
    "        model=model,\n",
    "        submodules=submodules,\n",
    "        dictionaries=dictionaries,\n",
    "        metric_fn=metric_fn,\n",
    "        method=\"all-folded\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Embedding' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43msubmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtuple\u001b[39m\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/nnsight/envoy.py:294\u001b[0m, in \u001b[0;36mEnvoy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Envoy, Any]:\n\u001b[1;32m    285\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper method for underlying module's attributes.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m        Any: Attribute.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Embedding' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "type(submodules[0].output.shape) == tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43msubmodule_nnsight\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/.conda/lib/python3.11/site-packages/nnsight/envoy.py:273\u001b[0m, in \u001b[0;36mEnvoy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n",
      "\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Envoy:\n",
      "\u001b[1;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper method for underlying ModuleList getitem.\u001b[39;00m\n",
      "\u001b[1;32m    265\u001b[0m \n",
      "\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m        Envoy: Envoy.\u001b[39;00m\n",
      "\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sub_envoys\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EffectOut(effects={Embedding(50304, 512): SparseAct(act=tensor([[[0., -0., -0.,  ..., -0., -0., -0.],\n",
       "         [0., -0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[ 2.4042e-04],\n",
       "         [-8.6635e-05]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., -0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[-0.0026],\n",
       "         [-0.1496]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., -0., 0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., -0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[-0.0005],\n",
       "         [-0.2292]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., -0., -0.,  ..., 0., -0., 0.],\n",
       "         [-0., -0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[-1.2706e-05],\n",
       "         [-4.3931e-01]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[-0., -0., -0.,  ..., -0., 0., -0.],\n",
       "         [-0., -0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[ 3.0721e-05],\n",
       "         [-1.3793e-01]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[-0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [-0., -0., 0.,  ..., -0., -0., 0.]]], device='cuda:0'), resc=tensor([[[-1.9471e-05],\n",
       "         [ 6.7521e-02]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [-0., -0., 0.,  ..., -0., -0., -0.]]], device='cuda:0'), resc=tensor([[[ 0.0000],\n",
       "         [-0.1269]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-0., 0., -0.,  ..., 0., -0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., -0.]]], device='cuda:0'), resc=tensor([[[-0.0001],\n",
       "         [-0.0002]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-0., -0., 0.,  ..., 0., 0., 0.],\n",
       "         [-0., 0., 0.,  ..., -0., 0., -0.]]], device='cuda:0'), resc=tensor([[[-0.0004],\n",
       "         [-0.3620]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0., 0., -0.,  ..., 0., -0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., -0.]]], device='cuda:0'), resc=tensor([[[ 0.0004],\n",
       "         [-0.1005]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-0.0000, 0.0000, 0.0000,  ..., 0.0000, -0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.0000, -0.0000]]],\n",
       "       device='cuda:0'), resc=tensor([[[6.5569e-05],\n",
       "         [6.7571e-02]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-0., 0., -0.,  ..., -0., -0., 0.],\n",
       "         [-0., 0., 0.,  ..., -0., 0., 0.]]], device='cuda:0'), resc=tensor([[[-2.6246e-05],\n",
       "         [ 7.9805e-02]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[ 0.0000],\n",
       "         [-0.2268]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., -0.,  ..., -0., 0., -0.],\n",
       "         [-0., 0., 0.,  ..., 0., -0., 0.]]], device='cuda:0'), resc=tensor([[[0.0007],\n",
       "         [0.1584]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[-0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[-5.1736e-05],\n",
       "         [-2.0416e-01]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., -0., -0.,  ..., 0., 0., 0.],\n",
       "         [-0., 0., -0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[-2.1747e-04],\n",
       "         [-3.1812e-01]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., -0., -0.],\n",
       "         [-0., -0., 0.,  ..., -0., 0., 0.]]], device='cuda:0'), resc=tensor([[[-4.9398e-05],\n",
       "         [-2.9965e-01]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[-0., -0., 0.,  ..., 0., 0., 0.],\n",
       "         [-0., -0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), resc=tensor([[[2.3173e-06],\n",
       "         [1.9775e-01]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., -0., -0.,  ..., 0., -0., -0.]]], device='cuda:0'), resc=tensor([[[0.0000],\n",
       "         [0.0391]]], device='cuda:0'))}, deltas={Embedding(50304, 512): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-1.7434e-06, -2.1011e-06, -3.0585e-06,  ..., -9.5628e-06,\n",
       "          -5.5507e-06,  5.6848e-06],\n",
       "         [-7.9703e-06,  9.0115e-05, -6.2596e-05,  ..., -4.4759e-06,\n",
       "          -1.2225e-05,  3.5821e-05]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 1.2295e-03,  1.0969e-04,  4.7478e-04,  ...,  2.6513e-05,\n",
       "           2.3868e-05,  1.1784e-04],\n",
       "         [ 3.5584e-02, -2.7632e-02,  1.0209e-02,  ...,  1.4254e-02,\n",
       "          -7.4929e-04, -2.0682e-02]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 6.7033e-05, -1.2019e-04, -1.2943e-04,  ..., -4.0758e-04,\n",
       "          -1.0108e-04, -8.7000e-05],\n",
       "         [-5.3088e-02,  3.3522e-02,  3.6754e-02,  ...,  5.4737e-02,\n",
       "           2.3242e-02,  1.9638e-02]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 2.0418e-05,  3.1833e-04,  1.8296e-04,  ..., -2.2636e-04,\n",
       "           4.1255e-04, -5.3406e-05],\n",
       "         [-9.1187e-02, -1.0268e-01,  1.0619e-02,  ..., -1.2798e-03,\n",
       "           2.2201e-02,  6.4711e-02]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 6.2704e-04,  2.8152e-05, -6.3043e-04,  ..., -2.0289e-04,\n",
       "           1.5965e-04,  3.0991e-04],\n",
       "         [-1.0805e-02, -1.6525e-02,  3.1383e-02,  ...,  3.4347e-02,\n",
       "          -1.7530e-02, -4.4814e-02]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 6.3516e-05,  2.4714e-05,  2.7835e-05,  ..., -6.2998e-05,\n",
       "           1.4200e-04, -2.4216e-04],\n",
       "         [-2.9645e-03,  8.7031e-03,  2.9155e-02,  ..., -1.1468e-02,\n",
       "          -1.1831e-02, -1.5302e-02]]], device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-8.0943e-05,  8.2076e-05, -7.1228e-05,  ...,  1.3387e-04,\n",
       "           6.8784e-05, -4.1127e-05],\n",
       "         [ 1.7173e-02,  1.3760e-02,  1.3953e-02,  ..., -6.2371e-02,\n",
       "           1.5026e-02,  1.6973e-02]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 4.3273e-05,  5.7638e-05,  1.8895e-05,  ..., -3.5577e-06,\n",
       "           5.9813e-05, -1.3754e-05],\n",
       "         [-2.1598e-04, -1.3316e-04,  3.4882e-04,  ..., -7.0399e-04,\n",
       "           8.6308e-05, -5.6372e-04]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 3.4273e-06, -1.6519e-04, -5.7176e-05,  ...,  3.2205e-05,\n",
       "           1.0198e-04,  1.8156e-04],\n",
       "         [ 9.8734e-02, -1.2345e-01,  8.9980e-02,  ...,  1.3294e-01,\n",
       "           5.5292e-03,  5.7108e-02]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-0.0004, -0.0011,  0.0002,  ..., -0.0003, -0.0003,  0.0007],\n",
       "         [-0.1246, -0.0507,  0.0325,  ..., -0.0858,  0.0012,  0.0229]]],\n",
       "       device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0408,  0.0000,  0.0000]]],\n",
       "       device='cuda:0'), res=tensor([[[ 7.0572e-04,  1.9355e-04, -5.5158e-04,  ...,  6.1128e-04,\n",
       "           3.7146e-04, -3.1751e-04],\n",
       "         [-7.1708e-02,  3.3408e-02, -8.5306e-02,  ...,  1.4926e-01,\n",
       "          -2.9796e-01, -2.9600e-03]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-9.2983e-05,  7.8872e-05,  3.1102e-04,  ...,  2.4927e-04,\n",
       "           7.9250e-04,  4.1986e-04],\n",
       "         [ 2.3080e-01,  9.9752e-02, -9.4597e-02,  ..., -1.1945e-01,\n",
       "          -2.4318e-01, -1.2357e-01]]], device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 1.0400e-03,  3.1233e-05, -4.5300e-04,  ...,  1.6642e-03,\n",
       "           4.4203e-04,  5.7650e-04],\n",
       "         [-9.9270e-02,  6.2066e-02,  4.9818e-02,  ..., -1.8216e-01,\n",
       "          -8.9811e-02,  1.2195e-01]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-4.0439e-04,  1.1261e-04, -1.0577e-04,  ..., -2.7381e-05,\n",
       "          -5.2154e-06, -1.2590e-04],\n",
       "         [ 8.0364e-02, -2.7043e-02,  2.1761e-02,  ..., -3.1473e-02,\n",
       "           1.7378e-02,  4.2622e-02]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-3.0833e-04,  1.6272e-04,  1.7177e-04,  ..., -4.3243e-05,\n",
       "           4.5322e-05,  2.2054e-05],\n",
       "         [ 3.2345e-02, -1.6105e-01,  3.2368e-02,  ...,  7.9198e-02,\n",
       "          -1.7467e-01, -1.3054e-03]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-3.3736e-05, -1.2738e-03,  3.3237e-04,  ..., -6.0987e-04,\n",
       "           6.1917e-04, -9.1118e-04],\n",
       "         [-4.8359e-02, -2.2146e-01,  1.7907e-01,  ...,  6.2926e-02,\n",
       "           2.0708e-01,  1.1159e-01]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 4.3488e-04,  1.2919e-04, -3.5584e-04,  ..., -9.8705e-04,\n",
       "          -4.5681e-04, -4.9214e-04],\n",
       "         [-9.1875e-02, -1.7691e-01,  8.5543e-02,  ...,  1.8579e-01,\n",
       "          -1.7366e-01,  3.9695e-02]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[ 1.5400e-04,  5.6976e-04, -8.7625e-04,  ..., -2.4470e-03,\n",
       "          -1.0872e-04,  1.9868e-03],\n",
       "         [-4.0575e-02, -9.5457e-02, -1.2356e-01,  ...,  4.9882e-02,\n",
       "          -2.8054e-01,  3.1652e-02]]], device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0'), res=tensor([[[-1.5607e-03, -7.4673e-04, -1.2112e-04,  ..., -8.6522e-04,\n",
       "          -1.1189e-03, -1.1849e-03],\n",
       "         [-1.6059e-01, -2.4779e-01, -8.7715e-02,  ..., -2.1264e-01,\n",
       "          -6.7239e-01,  3.8767e-02]]], device='cuda:0'))}, grads={Embedding(50304, 512): SparseAct(act=tensor([[[ 4.5083, -1.4007, -3.7402,  ..., -1.3834, -0.5472, -1.0459],\n",
       "         [ 0.4672, -0.1359,  2.6793,  ...,  0.3374,  0.9294,  2.2711]]],\n",
       "       device='cuda:0'), res=tensor([[[ 1.6021, -0.1895, -0.9301,  ...,  0.4246,  1.3197,  2.3362],\n",
       "         [-0.4859, -0.6743,  0.5672,  ...,  0.5755,  0.0234,  1.9784]]],\n",
       "       device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[ 0.0271,  0.0726,  0.1567,  ...,  0.1244,  0.1453,  0.1656],\n",
       "         [ 0.0390, -0.0456,  0.3940,  ...,  0.0736,  0.2475,  0.2602]]],\n",
       "       device='cuda:0'), res=tensor([[[-0.0456, -0.1975, -0.1499,  ...,  0.0015, -0.1810, -0.1235],\n",
       "         [-0.2821,  0.1025, -0.0613,  ...,  0.0990, -0.3971, -0.2772]]],\n",
       "       device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[ 0.0884, -0.0491,  0.1758,  ...,  0.0923, -0.0289,  0.0070],\n",
       "         [-0.3424, -0.1507, -0.1368,  ...,  0.1010,  0.0161,  0.1736]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0096,  0.0282, -0.0028,  ...,  0.0773,  0.0225,  0.0005],\n",
       "         [ 0.0845,  0.1123, -0.2455,  ...,  0.1372, -0.3506, -0.3775]]],\n",
       "       device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[ 0.0091, -0.0133, -0.0057,  ...,  0.0123, -0.0244,  0.0496],\n",
       "         [-0.1286, -0.3169,  0.0774,  ...,  0.3442,  0.1114,  0.0865]]],\n",
       "       device='cuda:0'), res=tensor([[[-0.0008, -0.0135,  0.0157,  ...,  0.0174, -0.0175, -0.0029],\n",
       "         [ 0.0438,  0.0289, -0.2799,  ...,  0.0940, -0.4212, -0.3449]]],\n",
       "       device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[-0.0069, -0.0129, -0.0050,  ..., -0.0065,  0.0082, -0.0108],\n",
       "         [-0.0861, -0.0585,  0.0270,  ...,  0.0162,  0.3966,  0.2370]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0055, -0.0048,  0.0007,  ...,  0.0057, -0.0014, -0.0012],\n",
       "         [ 0.0023,  0.0587, -0.3194,  ...,  0.1294, -0.3630, -0.2111]]],\n",
       "       device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[-0.0040,  0.0118,  0.0079,  ...,  0.0029,  0.0024,  0.0021],\n",
       "         [-0.3355, -0.1771,  0.0346,  ..., -0.2057, -0.0392,  0.1174]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0095,  0.0056, -0.0105,  ..., -0.0073, -0.0031,  0.0006],\n",
       "         [ 0.1386,  0.1392, -0.2531,  ..., -0.0326, -0.2826, -0.1583]]],\n",
       "       device='cuda:0')), GPTNeoXAttention(\n",
       "  (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "): SparseAct(act=tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0072, -0.0792,  0.0615,  ..., -0.1494, -0.0266, -0.1736]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0779,  0.1571, -0.0645,  ..., -0.0790, -0.2028, -0.0312]]],\n",
       "       device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-0.0600,  0.2971, -0.2487,  ...,  0.0658, -0.0026,  0.3380],\n",
       "         [ 0.1974,  0.1961,  0.2856,  ...,  0.4187,  0.2601, -0.1314]]],\n",
       "       device='cuda:0'), res=tensor([[[-0.0456, -0.1975, -0.1499,  ...,  0.0015, -0.1810, -0.1235],\n",
       "         [-0.2821,  0.1025, -0.0613,  ...,  0.0990, -0.3971, -0.2772]]],\n",
       "       device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-0.0351, -0.0051,  0.0638,  ...,  0.0102,  0.0004,  0.0870],\n",
       "         [-0.1780,  0.0088,  0.0951,  ..., -0.0612,  0.3389, -0.0060]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0096,  0.0282, -0.0028,  ...,  0.0773,  0.0225,  0.0005],\n",
       "         [ 0.0845,  0.1123, -0.2455,  ...,  0.1372, -0.3506, -0.3775]]],\n",
       "       device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[ 0.0136,  0.0222, -0.0226,  ...,  0.0404, -0.0014,  0.0043],\n",
       "         [ 0.2133,  0.1354,  0.0252,  ...,  0.0973,  0.0056, -0.2043]]],\n",
       "       device='cuda:0'), res=tensor([[[-0.0008, -0.0135,  0.0157,  ...,  0.0174, -0.0175, -0.0029],\n",
       "         [ 0.0438,  0.0289, -0.2799,  ...,  0.0940, -0.4212, -0.3449]]],\n",
       "       device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-0.0010,  0.0066,  0.0081,  ...,  0.0064, -0.0069,  0.0011],\n",
       "         [ 0.0524,  0.1069,  0.2013,  ..., -0.0209,  0.0597, -0.0530]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0055, -0.0048,  0.0007,  ...,  0.0057, -0.0014, -0.0012],\n",
       "         [ 0.0023,  0.0587, -0.3194,  ...,  0.1294, -0.3630, -0.2111]]],\n",
       "       device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[-5.2639e-04,  4.6717e-03, -1.2879e-04,  ..., -1.2724e-02,\n",
       "          -8.0783e-03,  1.1961e-04],\n",
       "         [-9.9303e-02,  6.4684e-02,  9.6526e-02,  ..., -3.2721e-02,\n",
       "           2.3648e-02,  1.5044e-01]]], device='cuda:0'), res=tensor([[[ 0.0095,  0.0056, -0.0105,  ..., -0.0073, -0.0031,  0.0006],\n",
       "         [ 0.1386,  0.1392, -0.2531,  ..., -0.0326, -0.2826, -0.1583]]],\n",
       "       device='cuda:0')), GPTNeoXMLP(\n",
       "  (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (act): GELUActivation()\n",
       "): SparseAct(act=tensor([[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0577, 0.0543, 0.0690,  ..., 0.0586, 0.1071, 0.1249]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0779,  0.1571, -0.0645,  ..., -0.0790, -0.2028, -0.0312]]],\n",
       "       device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[ 0.2224,  0.0490, -0.0590,  ..., -0.1441,  0.0357, -0.1747],\n",
       "         [-0.0441,  0.0293,  0.2068,  ...,  0.0695, -0.1309,  0.2081]]],\n",
       "       device='cuda:0'), res=tensor([[[-0.0456, -0.1975, -0.1499,  ...,  0.0015, -0.1810, -0.1235],\n",
       "         [-0.2821,  0.1025, -0.0613,  ...,  0.0990, -0.3971, -0.2772]]],\n",
       "       device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[-0.0540,  0.0114,  0.0732,  ...,  0.0176,  0.0037,  0.0503],\n",
       "         [ 0.0353,  0.2038,  0.1505,  ...,  0.0202,  0.1711,  0.0487]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0096,  0.0282, -0.0028,  ...,  0.0773,  0.0225,  0.0005],\n",
       "         [ 0.0845,  0.1123, -0.2455,  ...,  0.1372, -0.3506, -0.3775]]],\n",
       "       device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[ 0.0069, -0.0086, -0.0025,  ...,  0.0089,  0.0004,  0.0015],\n",
       "         [-0.3057,  0.2167, -0.0023,  ...,  0.1546,  0.3226,  0.0106]]],\n",
       "       device='cuda:0'), res=tensor([[[-0.0008, -0.0135,  0.0157,  ...,  0.0174, -0.0175, -0.0029],\n",
       "         [ 0.0438,  0.0289, -0.2799,  ...,  0.0940, -0.4212, -0.3449]]],\n",
       "       device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[ 0.0035,  0.0145,  0.0063,  ...,  0.0021, -0.0105, -0.0044],\n",
       "         [-0.0759, -0.1385,  0.1156,  ..., -0.0399,  0.1497,  0.1044]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0055, -0.0048,  0.0007,  ...,  0.0057, -0.0014, -0.0012],\n",
       "         [ 0.0023,  0.0587, -0.3194,  ...,  0.1294, -0.3630, -0.2111]]],\n",
       "       device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[-0.0094, -0.0092,  0.0022,  ...,  0.0083,  0.0098,  0.0072],\n",
       "         [-0.6162, -0.1322,  0.0797,  ...,  0.1572,  0.0096,  0.0169]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0095,  0.0056, -0.0105,  ..., -0.0073, -0.0031,  0.0006],\n",
       "         [ 0.1386,  0.1392, -0.2531,  ..., -0.0326, -0.2826, -0.1583]]],\n",
       "       device='cuda:0')), GPTNeoXLayer(\n",
       "  (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (attention): GPTNeoXAttention(\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "    (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "    (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (mlp): GPTNeoXMLP(\n",
       "    (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (act): GELUActivation()\n",
       "  )\n",
       "): SparseAct(act=tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0928, -0.1407, -0.1332,  ...,  0.0747, -0.3214, -0.1694]]],\n",
       "       device='cuda:0'), res=tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0779,  0.1571, -0.0645,  ..., -0.0790, -0.2028, -0.0312]]],\n",
       "       device='cuda:0'))}, total_effect=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
